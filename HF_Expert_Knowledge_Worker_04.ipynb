{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSvntt1C+DTJGJBto46ojW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarunku/open_llm/blob/main/HF_Expert_Knowledge_Worker_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoR3vW2ukFAH"
      },
      "outputs": [],
      "source": [
        "!pip install -qU --upgrade fsspec==2025.3.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu-cu11 openpyxl pacmap datasets langchain-community ragatouille"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GXfwmLxzkR-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Very Important -  Restart Session"
      ],
      "metadata": {
        "id": "uW1QZhVnkcvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, AutoModelForSpeechSeq2Seq, pipeline\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy"
      ],
      "metadata": {
        "id": "lqGZu517kkTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Base"
      ],
      "metadata": {
        "id": "eOWNSCtClUkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to google drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "Ii0fjUCplagw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface login\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "MMdtJhz_lijv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overall Workflow**\n",
        "1. **Load and chunk markdown documents** from a specified directory.\n",
        "2. **Convert text chunks into numerical embeddings** using a pre-trained model.\n",
        "3. **Store/retrieve embeddings in a FAISS vector database** for efficient similarity search.\n"
      ],
      "metadata": {
        "id": "ophI_wrLu8Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Load files**  \n",
        "   - It reads all files from a specified directory (`raw_knowledge_base_path`).\n",
        "   - Uses `DirectoryLoader` from **LangChain** to load these documents.\n",
        "   - Attaches metadata (`doc_type`) to identify which subfolder a document belongs to.\n",
        "\n",
        "2. **Split documents into chunks**  \n",
        "   - Uses `RecursiveCharacterTextSplitter` to break documents into chunks of ~1000 characters.\n",
        "   - Overlaps 100 characters between chunks to preserve context.\n",
        "   - Uses **Markdown-specific separators** (e.g., headings `#`, code blocks ```).\n",
        "   - Returns the chunked documents."
      ],
      "metadata": {
        "id": "LaNWeWMDt78h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits documents into smaller chunks for embedding and retrieval.\n",
        "\n",
        "def chunking(raw_knowledge_base_path , filter):\n",
        "  # Read in documents using LangChain's loaders\n",
        "  # Take everything in all the sub-folders of our knowledgebase\n",
        "\n",
        "  folders = glob.glob(raw_knowledge_base_path)\n",
        "\n",
        "  text_loader_kwargs = {'encoding': 'utf-8'}\n",
        "\n",
        "  documents = []\n",
        "  for folder in folders:\n",
        "      doc_type = os.path.basename(folder)\n",
        "      loader = DirectoryLoader(folder, glob= filter, loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
        "      folder_docs = loader.load()\n",
        "      for doc in folder_docs:\n",
        "          doc.metadata[\"doc_type\"] = doc_type\n",
        "          documents.append(doc)\n",
        "\n",
        "  # We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
        "  # This list is taken from LangChain's MarkdownTextSplitter class\n",
        "  MARKDOWN_SEPARATORS = [\n",
        "      \"\\n#{1,6} \",\n",
        "      \"```\\n\",\n",
        "      \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "      \"\\n---+\\n\",\n",
        "      \"\\n___+\\n\",\n",
        "      \"\\n\\n\",\n",
        "      \"\\n\",\n",
        "      \" \",\n",
        "      \"\",\n",
        "  ]\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=1000,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
        "      chunk_overlap=100,  # The number of characters to overlap between chunks\n",
        "      add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
        "      strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
        "      separators=MARKDOWN_SEPARATORS,\n",
        "  )\n",
        "\n",
        "  chunks = text_splitter.split_documents(documents)\n",
        "  return chunks\n"
      ],
      "metadata": {
        "id": "72tsMluIr7rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embedding and Storing in a Vector Database**\n",
        "### **Purpose:**  \n",
        "Converts text chunks into vector embeddings and stores them for similarity search.\n",
        "\n",
        "### **Process:**\n",
        "1. **Define Embedding Model**  \n",
        "   - Uses `HuggingFaceEmbeddings` with the `thenlper/gte-small` model.\n",
        "   - Runs embeddings on **GPU (`cuda`)**.\n",
        "   - Normalizes embeddings for **cosine similarity**.\n",
        "\n",
        "2. **Check for Existing Vector Database**  \n",
        "   - If the FAISS vector database file (`VECTOR_DB_PATH`) exists, it loads it.\n",
        "   - Otherwise, it:\n",
        "     - Calls `chunking(RAW_KB_PATH)` to get document chunks.\n",
        "     - Converts chunks into embeddings.\n",
        "     - Stores the vector database locally."
      ],
      "metadata": {
        "id": "8Szyse6oujjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "VECTOR_DB_PATH = \"/content/drive/MyDrive/__tmp/llms/knowledge_vector_db\"  # Path to store the database\n",
        "RAW_KB_PATH = \"/content/drive/MyDrive/__tmp/llms/knowledge-base/*\"  # Path to folder where files are stored\n",
        "RAW_KB_DOC_FILTER = \"**/*.md\"\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        ")\n",
        "\n",
        "# Check if the database file already exists\n",
        "if os.path.exists(VECTOR_DB_PATH):\n",
        "    print(\"Loading existing vector database...\")\n",
        "    KNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(VECTOR_DB_PATH, embedding_model, allow_dangerous_deserialization=True) # Changed EMBEDDING_MODEL_NAME to embedding_model\n",
        "else:\n",
        "    print(\"Creating new vector database...\")\n",
        "    KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "        chunking(RAW_KB_PATH, RAW_KB_DOC_FILTER), embedding_model, distance_strategy=DistanceStrategy.COSINE # Changed EMBEDDING_MODEL_NAME to embedding_model\n",
        "    )\n",
        "    print(\"Saving vector database to Google Drive...\")\n",
        "    KNOWLEDGE_VECTOR_DATABASE.save_local(VECTOR_DB_PATH)\n",
        "    print(\"Vector database saved successfully!\")\n",
        ""
      ],
      "metadata": {
        "id": "JM_uNyqLpntr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This code **embeds a user query** and retrieves the **most relevant documents** using FAISS.\n",
        "- **How it works**:\n",
        "  1. Converts the query into a vector embedding.\n",
        "  2. Searches for the **top 5 closest matches** in the vector database.\n",
        "  3. Returns the **most relevant document chunks**.\n",
        "  4. Measures **retrieval speed**.\n",
        "  5. Prints metadata of retrieved documents.\n",
        "- **Use Cases**:\n",
        "  - AI-powered **search and recommendation systems**.\n",
        "  - **Chatbots** that provide information based on stored knowledge.\n",
        "  - **Automated document retrieval** in knowledge bases.\n"
      ],
      "metadata": {
        "id": "0GsHLc8LmBhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed a user query in the same space\n",
        "import time\n",
        "start_time = time.time()  # Record the start time\n",
        "\n",
        "user_query = \"Who is Lancaster?\"\n",
        "#query_vector = embedding_model.embed_query(user_query)\n",
        "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
        "\n",
        "end_time = time.time()  # Record the end time\n",
        "retrieval_time = end_time - start_time  # Calculate the time difference\n",
        "\n",
        "print(f\"Retrieval time: {retrieval_time:.4f} seconds\")\n",
        "\n",
        "print('\\n'.join([str(doc.metadata) for doc in retrieved_docs]))"
      ],
      "metadata": {
        "id": "ZoNP3CF-l9Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Creates a **text-generation pipeline** for **efficient model inference**.\n",
        "- **Key Parameters:**\n",
        "  - **`task=\"text-generation\"`** â†’ Specifies the type of model.\n",
        "  - **`do_sample=True`** â†’ Uses random sampling instead of greedy decoding.\n",
        "  - **`temperature=0.2`** â†’ Low value makes responses more deterministic.\n",
        "  - **`repetition_penalty=1.1`** â†’ Prevents repetition by penalizing repeated words.\n",
        "  - **`return_full_text=False`** â†’ Only returns generated text (excludes input prompt).\n",
        "  - **`max_new_tokens=500`** â†’ Limits generation length to 500 tokens.\n",
        "  \n",
        "This function **loads a quantized text-generation model** using **Hugging Face Transformers** and **bitsandbytes (bnb) for efficient inference**.  \n",
        "\n",
        "It sets up a **causal language model (CLM)** with optimized **4-bit quantization** to reduce memory usage while maintaining accuracy. The function **returns a text-generation pipeline and tokenizer**, ready to generate text.\n",
        "\n",
        "- **Quantization** reduces memory and speeds up inference, especially for large models.\n",
        "- **Key settings:**\n",
        "  - **`load_in_4bit=True`** â†’ Loads the model in **4-bit** mode, significantly reducing VRAM usage.\n",
        "  - **`bnb_4bit_use_double_quant=True`** â†’ Uses **double quantization** for better compression.\n",
        "  - **`bnb_4bit_quant_type=\"nf4\"`** â†’ Uses **NF4 (Normal Float 4)**, a specialized data format optimized for AI.\n",
        "  - **`bnb_4bit_compute_dtype=torch.bfloat16`** â†’ Uses **bfloat16** (faster and more stable than float16).\n",
        "\n",
        "ðŸš€ **Why 4-bit quantization?**  \n",
        "- Reduces memory usage (helps run large models on consumer GPUs).  \n",
        "- Allows **faster inference** without major accuracy loss."
      ],
      "metadata": {
        "id": "TJA63gYEmyOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_pipeline(READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"):\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "  )\n",
        "  model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "  return pipeline(\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      task=\"text-generation\",\n",
        "      do_sample=True,\n",
        "      temperature=0.2,\n",
        "      repetition_penalty=1.1,\n",
        "      return_full_text=False,\n",
        "      max_new_tokens=500,\n",
        "  ), tokenizer"
      ],
      "metadata": {
        "id": "8KzpDlSImx-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline, tokenizer = load_pipeline(\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "# pipeline, tokenizer = load_pipeline(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
        "pipeline, tokenizer = load_pipeline(\"Qwen/Qwen2-7B-Instruct\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QGVWC3CWnRrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be found from the context, then do not give an answer, but say i dont know.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")"
      ],
      "metadata": {
        "id": "pmfcehksnDrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**answers a user query using a Retrieval-Augmented Generation (RAG) pipeline** by:  \n",
        "\n",
        "1. **Retrieving relevant documents** from a **FAISS vector database**.  \n",
        "2. **Optionally reranking** the retrieved documents using a **pretrained RAG model**.  \n",
        "3. **Building a final prompt** by including the retrieved documents.  \n",
        "4. **Generating an answer** using a **text-generation model (LLM)**.  \n",
        "\n",
        "### **Key Steps**\n",
        "1. **Retrieve Documents** â†’ Finds `num_retrieved_docs` most relevant documents using FAISS.  \n",
        "2. **Rerank (Optional)** â†’ If a `reranker` model is provided, it refines the ranking.  \n",
        "3. **Format the Prompt** â†’ Constructs a final prompt using retrieved documents.  \n",
        "4. **Generate Answer** â†’ Passes the prompt to the **LLM** for response generation.  \n",
        "5. **Return** â†’ The function outputs the **generated answer** and the **final set of documents used**.  \n"
      ],
      "metadata": {
        "id": "h4p_6B9PwxBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "from transformers import AutoTokenizer, Pipeline\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: Pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 5,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    #print(final_prompt)\n",
        "\n",
        "    # Redact an answer\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs"
      ],
      "metadata": {
        "id": "rzESiv3sobqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what Emily Carter was doing during 2019-2021?\"\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question, pipeline, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\n"
      ],
      "metadata": {
        "id": "ZVPmD9cwotR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "id": "EtBTvg6TpH3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is the capital city of India?\"\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question, pipeline, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\n"
      ],
      "metadata": {
        "id": "yPRf76U52Fg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "id": "3Nz4ofiN2MZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}