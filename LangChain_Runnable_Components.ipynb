{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLcTBp8njLw4SoJaeMHCu2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarunku/open_llm/blob/main/LangChain_Runnable_Components.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knNvL5vWNeSg"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-openai langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RunnableLambda** is a class in LangChain\n",
        "\n",
        "that allows you to wrap a Python function as a runnable component in a chain. It essentially turns your function into a building block that can be connected with other components to create complex workflows.\n",
        "\n",
        "\n",
        "**Why it's used:**\n",
        "\n",
        "Flexibility: It enables you to integrate custom logic and functions into your LangChain chains.\n",
        "Data manipulation: You can use it to pre-process or post-process data within the chain.\n",
        "Dynamic behavior: It allows you to create chains that adapt to different inputs or conditions.\n",
        "\n",
        "\n",
        "# **RunnableSequence** a class in LangChain\n",
        "that allows you to define a sequence of runnable components that will be executed in order. It's like creating a pipeline where the output of one component becomes the input of the next.\n",
        "\n",
        "Why it's used:\n",
        "\n",
        "Workflow orchestration: It helps you define and manage complex workflows involving multiple steps.\n",
        "Modular design: It promotes code reusability by allowing you to combine and reuse individual components.\n",
        "Control flow: It gives you control over the order of execution and data flow between components.\n",
        "\n",
        "\n",
        "# To creates a sequence:\n",
        "\n",
        "*   step1 (the RunnableLambda wrapping get_random_country) is executed first.\n",
        "*   step2 (asking for the popular city) is executed next, using the output of step1 as input.\n",
        "*   step3 (asking for things to do) is executed last, using the output of step2 as input.\n",
        "\n",
        "By using RunnableLambda and RunnableSequence, code becomes more modular, flexible, and easier to understand and maintain."
      ],
      "metadata": {
        "id": "BYrFX5qDW4e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "def get_random_country(country=None):\n",
        "  \"\"\"Returns a random country name from the list.\n",
        "\n",
        "  Args:\n",
        "    countries: A list of country names.\n",
        "\n",
        "  Returns:\n",
        "    A random country name from the list.\n",
        "  \"\"\"\n",
        "  country_list = [\"United States\", \"Canada\", \"Mexico\", \"Brazil\", \"India\", \"China\", \"Japan\"]\n",
        "  return random.choice(country_list)\n",
        "\n",
        "template_1 = \"What is the most popular city in {country} for tourists? Just return the name of the city\"\n",
        "template_2 = \"What are the top three things to do in this: {city} for tourists. Just return the answer as three bullet points.\"\n",
        "\n",
        "prompt_1 = ChatPromptTemplate.from_template(template_1)\n",
        "prompt_2 = ChatPromptTemplate.from_template(template_2)\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-4o-mini\", openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "\n",
        "#runnable_sequence = RunnableLambda(get_random_country) | prompt_1 | model | output_parser | prompt_2 | model | output_parser\n",
        "\n",
        "# Define the individual steps as Runnable objects\n",
        "step1 = RunnableLambda(get_random_country)\n",
        "step2 = prompt_1 | model | output_parser  # Combine prompt, model, and parser\n",
        "step3 = prompt_2 | model | output_parser  # Combine prompt, model, and parser\n",
        "\n",
        "#runnable_sequence = step1 | step2 | step3\n",
        "\n",
        "# Initialize RunnableSequence\n",
        "runnable_sequence = RunnableSequence(\n",
        "    first=step1,\n",
        "    middle=[step2],  # Put step2 in a list for the \"middle\" argument\n",
        "    last=step3\n",
        ")\n",
        "\n",
        "runnable_sequence.invoke(None)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J9rb0DUfQyBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}