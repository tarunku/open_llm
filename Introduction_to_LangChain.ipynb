{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNo2JUC521xIQg3zgrUcByr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarunku/open_llm/blob/main/Introduction_to_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Basics: Building a Simple Translation Chain\n",
        "\n",
        "This notebook demonstrates the fundamental concepts of LangChain by building a basic translation application. We'll use the OpenAI language model (gpt-4o-mini) and an opensource model (HuggingFaceH4/zephyr-7b-beta) to translate text from English to French.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "1. **Prompt Templates:** We define prompt templates to structure the interaction with the language model. This involves:\n",
        "    - A system message that sets the context (e.g., \"You are a helpful assistant...\").\n",
        "    - A human message that provides the input text to be translated.\n",
        "\n",
        "2. **LLMChain:** We create an LLMChain object that links the language model and the prompt template. This enables us to easily run the translation task.\n",
        "\n",
        "3. **Input and Output:** We format the prompt with specific input values (source and target language, text) and use `chain.run()` or `chain.invoke()` to get the translated output.\n",
        "\n",
        "**How this shows basic LangChain object formation:**\n",
        "\n",
        "* **Combining Prompts and LLMs:** The core of LangChain is connecting language models with prompts. This notebook showcases how to create a prompt template and link it to a language model using the `LLMChain` class.\n",
        "\n",
        "* **Flexible Input and Output:** The `LLMChain` allows you to provide structured input (using dictionaries) and receive structured output, making it easy to integrate with other applications.\n",
        "\n",
        "* **Building Blocks for More Complex Chains:** This simple translation example serves as a foundation for building more complex LangChain applications. You can extend this concept to create chains for various tasks like summarization, question answering, and code generation.\n",
        "\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "This notebook provides a hands-on introduction to LangChain by demonstrating how to:\n",
        "\n",
        "- Define prompt templates.\n",
        "- Create an LLMChain to link a language model and a prompt.\n",
        "- Run the chain with specific input values to get the desired output.\n",
        "\n",
        "By understanding these basic concepts, you'll be well-equipped to explore more advanced features of LangChain and build powerful language-based applications."
      ],
      "metadata": {
        "id": "zU0alO5PtLic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies\n",
        "\n",
        "This code cell installs the required Python packages using `pip`:\n",
        "**Breakdown:**\n",
        "\n",
        "- `!pip install`: This is the command to install packages using `pip`, the Python package installer.\n",
        "- `-q`: This flag makes the installation quieter, reducing the output displayed during the process.\n",
        "- `-U`: This flag upgrades any existing packages to the latest versions.\n",
        "- `langchain`: Installs the core LangChain library.\n",
        "- `langchain-openai`: Installs the package for integration with OpenAI's models.\n",
        "- `langchain_community`: Installs community-developed LangChain extensions.\n",
        "- `transformers`: Installs the Hugging Face Transformers library for working with pre-trained language models.\n",
        "- `torch`: Installs the PyTorch library for deep learning.\n",
        "- `accelerate`: Installs the Hugging Face Accelerate library for training and inference optimizations.\n",
        "- `bitsandbytes`: Installs the bitsandbytes library, often used for quantization techniques to reduce model size."
      ],
      "metadata": {
        "id": "2kuLq8bN4DFn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Wd9rhF5uLAbX"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-openai langchain_community transformers torch accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries\n",
        "\n",
        "This section imports the required libraries for the LangChain application."
      ],
      "metadata": {
        "id": "oE5Pafcf381T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain import LLMChain\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chat_models import (\n",
        "    ChatOpenAI,\n",
        "    init_chat_model\n",
        ")\n",
        "\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TextStreamer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoModelForSpeechSeq2Seq,\n",
        "    pipeline\n",
        ")\n"
      ],
      "metadata": {
        "id": "JGsky00uzlU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "OVXiCgz6zq6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Pipeline: `load_pipeline` Function\n",
        "\n",
        "This function, `load_pipeline`, is responsible for loading and configuring a language model pipeline for text generation using Hugging Face Transformers and bitsandbytes for 4-bit quantization."
      ],
      "metadata": {
        "id": "KWljo5cV3zME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pipeline(READER_MODEL_NAME):\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "  )\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, device_map=\"auto\", quantization_config=bnb_config)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "\n",
        "  return pipeline(\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      task=\"text-generation\",\n",
        "      do_sample=True,\n",
        "      temperature=0.2,\n",
        "      repetition_penalty=1.1,\n",
        "      return_full_text=False,\n",
        "      max_new_tokens=1000,\n",
        "      streamer=streamer,\n",
        "  ), tokenizer"
      ],
      "metadata": {
        "id": "KwkMB0WozkHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Language Models\n",
        "\n",
        "This section initializes three different language models for use with LangChain:\n",
        "\n",
        "### 1. `first_llm`\n",
        "- **Purpose:** This line initializes an instance of the `ChatOpenAI` class to interact with OpenAI's GPT-4 Turbo model.\n",
        "- **Model:** It uses the `gpt-4o-mini` model.\n",
        "- **Authentication:** It retrieves your OpenAI API key using `userdata.get('OPENAI_API_KEY')`. Make sure you have stored your API key using `userdata.set('OPENAI_API_KEY', 'your_api_key')`.\n",
        "\n",
        "### 2. `second_llm`\n",
        "- **Purpose:** This section also initializes an OpenAI model using `init_chat_model` with additional configurations.\n",
        "- **Model:** It uses the same `gpt-4o-mini` model.\n",
        "- **Temperature:** It sets the `temperature` to 0 for deterministic output.\n",
        "- **Configuration:** It defines configurable fields for later modification.\n",
        "- **Prefix:** It uses the `config_prefix` \"first\" for chains with multiple models.\n",
        "- **Authentication:** It also uses your OpenAI API key for authentication.\n",
        "\n",
        "### 3. `os_llm`\n",
        "- **Purpose:** This part sets up an open-source language model using `HuggingFacePipeline`.\n",
        "- **Model:** It uses the `zephyr-7b-beta` model from Hugging Face.\n",
        "- **Pipeline:** It utilizes the `load_pipeline` function (defined elsewhere) to load and configure the model and its tokenizer, including 4-bit quantization and a text streamer.\n",
        "\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "The code sets up three language models: two from OpenAI (`first_llm` and `second_llm`) and one open-source model (`os_llm`). You can use these models in LangChain applications by assigning them to the `llm` parameter when creating chain objects."
      ],
      "metadata": {
        "id": "PVpumDm53hCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set up the OpenAI model,\n",
        "first_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "# Set up the OpenAI model\n",
        "second_llm = init_chat_model(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    openai_api_key=userdata.get('OPENAI_API_KEY'),\n",
        "    temperature=0,\n",
        "    configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n",
        "    config_prefix=\"first\",  # useful when you have a chain with multiple models\n",
        ")\n",
        "\n",
        "# Set up the Opensource model,\n",
        "pipeline, tokenizer = load_pipeline(\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "os_llm = HuggingFacePipeline(pipeline=pipeline)\n"
      ],
      "metadata": {
        "id": "XIwoxx6RSZjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Prompt Template\n",
        "\n",
        "This code snippet defines the prompt template that will be used to interact with the language model for translation.\n",
        "\n",
        "**1. System Message:**\n",
        "- `system_template`: This string defines the system message, which sets the context for the language model. It instructs the model to act as a helpful assistant for translation.\n",
        "- `system_message_prompt`: This creates a `SystemMessagePromptTemplate` object from the `system_template`. This object represents the system message within the prompt template.\n",
        "- `{input_language}` and `{output_language}`: These are placeholders that will be filled with specific language codes when the prompt is used.\n",
        "\n",
        "\n",
        "**2. Human Message:**\n",
        "- `human_template`: This string defines the human message, which will contain the text to be translated.\n",
        "- `human_message_prompt`:  This creates a `HumanMessagePromptTemplate` object from the `human_template`. This object represents the human message within the prompt template.\n",
        "- `{text}`: This is a placeholder that will be replaced with the actual text to be translated.\n",
        "\n",
        "\n",
        "**3. Combining Messages into a Chat Prompt:**\n",
        "- `chat_prompt`: This creates a `ChatPromptTemplate` object by combining the system message prompt and the human message prompt. This object represents the complete prompt template, including both system and human messages.\n",
        "\n",
        "**In essence, this code defines a structured prompt template that guides the language model to perform translation tasks by providing it with context and the text to be translated.**"
      ],
      "metadata": {
        "id": "sUUUAqWx3IlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n"
      ],
      "metadata": {
        "id": "DGPn-1ILsRGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# invoke\n",
        "\n",
        "**More Flexibility**: invoke provides more control and is better suited for complex scenarios. It expects an input dictionary that directly corresponds to the variables in your prompt template. This gives you finer-grained control over how the prompt is constructed.\n",
        "\n",
        "**Advanced Features**: It also handles callbacks and allows you to access intermediate values during the chain's execution, making it more versatile."
      ],
      "metadata": {
        "id": "Jzz5Wo1-rRVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#chain = LLMChain(llm = first_llm, prompt = chat_prompt);\n",
        "#chain = LLMChain(llm = second_llm, prompt = chat_prompt);\n",
        "chain = LLMChain(llm = os_llm, prompt = chat_prompt);\n",
        "\n",
        "result = chain.invoke({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love programming.\"})\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "7-0szB7OrRBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run\n",
        "\n",
        "**Simpler Interface:** run is designed for the most straightforward use cases. You provide it with a single input string, and it returns a single output string.\n",
        "\n",
        "**Behind the Scenes:** Under the hood, run converts your input string into an input dictionary, uses that to format the prompt template, and then calls the underlying language model with the formatted prompt. Finally, it extracts the relevant part of the language model's response and returns it."
      ],
      "metadata": {
        "id": "cZBbsCaKrgqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#chain = LLMChain(llm = first_llm, prompt = chat_prompt);\n",
        "#chain = LLMChain(llm = second_llm, prompt = chat_prompt);\n",
        "chain = LLMChain(llm = os_llm, prompt = chat_prompt);\n",
        "\n",
        "result = chain.run({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love programming.\"})\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "xcwD_WztSorf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDVEEtsSpFik"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}