{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsBpvL+fjGI+By9ycEF2I9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarunku/open_llm/blob/main/Introduction_to_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Basics: Building a Simple Translation Chain\n",
        "\n",
        "This notebook demonstrates the fundamental concepts of LangChain by building a basic translation application. We'll use the OpenAI language model (gpt-4o-mini) to translate text from English to French.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "1. **Prompt Templates:** We define prompt templates to structure the interaction with the language model. This involves:\n",
        "    - A system message that sets the context (e.g., \"You are a helpful assistant...\").\n",
        "    - A human message that provides the input text to be translated.\n",
        "\n",
        "2. **LLMChain:** We create an LLMChain object that links the language model and the prompt template. This enables us to easily run the translation task.\n",
        "\n",
        "3. **Input and Output:** We format the prompt with specific input values (source and target language, text) and use `chain.run()` or `chain.invoke()` to get the translated output.\n",
        "\n",
        "**How this shows basic LangChain object formation:**\n",
        "\n",
        "* **Combining Prompts and LLMs:** The core of LangChain is connecting language models with prompts. This notebook showcases how to create a prompt template and link it to a language model using the `LLMChain` class.\n",
        "\n",
        "* **Flexible Input and Output:** The `LLMChain` allows you to provide structured input (using dictionaries) and receive structured output, making it easy to integrate with other applications.\n",
        "\n",
        "* **Building Blocks for More Complex Chains:** This simple translation example serves as a foundation for building more complex LangChain applications. You can extend this concept to create chains for various tasks like summarization, question answering, and code generation.\n",
        "\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "This notebook provides a hands-on introduction to LangChain by demonstrating how to:\n",
        "\n",
        "- Define prompt templates.\n",
        "- Create an LLMChain to link a language model and a prompt.\n",
        "- Run the chain with specific input values to get the desired output.\n",
        "\n",
        "By understanding these basic concepts, you'll be well-equipped to explore more advanced features of LangChain and build powerful language-based applications."
      ],
      "metadata": {
        "id": "zU0alO5PtLic"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd9rhF5uLAbX"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain langchain_community langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "first_llm = init_chat_model(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    openai_api_key=userdata.get('OPENAI_API_KEY'),\n",
        "    temperature=0,\n",
        "    configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n",
        "    config_prefix=\"first\",  # useful when you have a chain with multiple models\n",
        ")"
      ],
      "metadata": {
        "id": "Rg-APaluOaI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "from langchain.chat_models import (\n",
        "    ChatOpenAI,\n",
        "    init_chat_model\n",
        ")\n",
        "\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate\n",
        ")\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up the OpenAI model\n",
        "first_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "\n",
        "second_llm = init_chat_model(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    openai_api_key=userdata.get('OPENAI_API_KEY'),\n",
        "    temperature=0,\n",
        "    configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n",
        "    config_prefix=\"first\",  # useful when you have a chain with multiple models\n",
        ")\n"
      ],
      "metadata": {
        "id": "XIwoxx6RSZjl"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n"
      ],
      "metadata": {
        "id": "DGPn-1ILsRGC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# invoke\n",
        "\n",
        "**More Flexibility**: invoke provides more control and is better suited for complex scenarios. It expects an input dictionary that directly corresponds to the variables in your prompt template. This gives you finer-grained control over how the prompt is constructed.\n",
        "\n",
        "**Advanced Features**: It also handles callbacks and allows you to access intermediate values during the chain's execution, making it more versatile."
      ],
      "metadata": {
        "id": "Jzz5Wo1-rRVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#chain = LLMChain(llm = first_llm, prompt = chat_prompt);\n",
        "chain = LLMChain(llm = second_llm, prompt = chat_prompt);\n",
        "\n",
        "result = chain.invoke({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love programming.\"})\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-0szB7OrRBj",
        "outputId": "059eb2af-71cc-40d7-b43f-29494eb4e32a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_language': 'English', 'output_language': 'French', 'text': \"J'aime la programmation.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run\n",
        "\n",
        "**Simpler Interface:** run is designed for the most straightforward use cases. You provide it with a single input string, and it returns a single output string.\n",
        "\n",
        "**Behind the Scenes:** Under the hood, run converts your input string into an input dictionary, uses that to format the prompt template, and then calls the underlying language model with the formatted prompt. Finally, it extracts the relevant part of the language model's response and returns it."
      ],
      "metadata": {
        "id": "cZBbsCaKrgqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#chain = LLMChain(llm = first_llm, prompt = chat_prompt);\n",
        "chain = LLMChain(llm = second_llm, prompt = chat_prompt);\n",
        "\n",
        "result = chain.run({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love programming.\"})\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcwD_WztSorf",
        "outputId": "e08a6e2e-aea5-4c37-d87e-1676daf4d9e0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J'aime la programmation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDVEEtsSpFik"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}