{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODnln4b1+mQPv79HPNi0xx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarunku/open_llm/blob/main/LangChain_for_Language_Model_Interactions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The notebook explores different options for language model interaction, highlighting the flexibility and capabilities of LangChain in integrating with various backends. It provides a starting point for comparing closed-source and open-source solutions and building more complex language model applications.*"
      ],
      "metadata": {
        "id": "bT6sJedWz3Oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct OpenAI API Call:\n",
        "\n",
        "**Uses the openai library to interact directly with OpenAI's GPT models (closed-source).** This approach is straightforward for basic requests.\n",
        "\n",
        "Requires an OpenAI API key stored securely in Google Colab user data."
      ],
      "metadata": {
        "id": "olXxrWkoza9W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gAtRB_iBht8N"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "openai = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "\n",
        "# Make a request to OpenAI's GPT model\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Say Hello, World!\"}]\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "viq93lbNh0f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain with OpenAI:\n",
        "\n",
        "**Utilizes the langchain library to simplify interactions with OpenAI's models (closed-source).** LangChain offers more flexibility and features for managing prompts, chains, memory, etc.\n",
        "\n",
        "Also requires an OpenAI API key."
      ],
      "metadata": {
        "id": "-zMhZXlPk-CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain langchain_community"
      ],
      "metadata": {
        "id": "bPBpJb2AlFcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up the OpenAI model\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "# Define messages (Chat history)\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Say Hello, World!\")\n",
        "]\n",
        "\n",
        "# Get response\n",
        "response = llm(messages)\n",
        "\n",
        "# Print the result\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "bXdEgI7JlB-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain with Open-Source LLMs:\n",
        "**Leverages langchain and transformers to access open-source LLMs like Zephyr and Llama 2.**\n",
        "\n",
        "This section demonstrates how to load and use these models within LangChain, offering an alternative to closed-source models.\n",
        "Includes instructions for installing necessary packages and authenticating with Hugging Face using a Hugging Face token."
      ],
      "metadata": {
        "id": "UX3fNv8Gyfv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain_community transformers torch accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "6NFb-bDBnbyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, AutoModelForSpeechSeq2Seq, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "def load_pipeline(READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"):\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "  )\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, device_map=\"auto\", quantization_config=bnb_config)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "\n",
        "  return pipeline(\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      task=\"text-generation\",\n",
        "      do_sample=True,\n",
        "      temperature=0.2,\n",
        "      repetition_penalty=1.1,\n",
        "      return_full_text=False,\n",
        "      max_new_tokens=1000,\n",
        "      streamer=streamer,\n",
        "  ), tokenizer\n"
      ],
      "metadata": {
        "id": "9CAgwWjgotIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline, tokenizer = load_pipeline(\"HuggingFaceH4/zephyr-7b-beta\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WusjL0UxqdZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import SystemMessage, HumanMessage"
      ],
      "metadata": {
        "id": "XO3_qwqbq_ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the pipeline with LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "\n",
        "# Generate response\n",
        "response = llm(\"Say Hello, World!\")\n",
        "\n",
        "# Print output\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Tk9mkjvRqk1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_VnKKNetuLLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline, tokenizer = load_pipeline(\"meta-llama/Llama-3.1-8B-Instruct\")"
      ],
      "metadata": {
        "id": "wPKBZ3R7rpfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the pipeline with LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "\n",
        "# Generate response\n",
        "response = llm(\"What is the capital city of India?\")\n",
        "\n",
        "# Print output\n",
        "print(response)"
      ],
      "metadata": {
        "id": "g04_nce3uXD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization with Langchain and Open Source LLM\n",
        "\n",
        "Demonstrates how to summarize information using Langchain and open-source LLMs.\n",
        "Includes steps on creating a prompt template, invoking the LLM chain, and printing the results."
      ],
      "metadata": {
        "id": "aHwzTV89y1gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "information = \"\"\"\n",
        "        Elon Reeve Musk (/ˈiːlɒn/; EE-lon; born June 28, 1971) is a businessman and investor. He is the founder, chairman, CEO, and CTO of SpaceX; angel investor, CEO, product architect and former chairman of Tesla, Inc.; owner, chairman and CTO of X Corp.; founder of the Boring Company and xAI; co-founder of Neuralink and OpenAI; and president of the Musk Foundation. He is the wealthiest person in the world, with an estimated net worth of US$232 billion as of December 2023, according to the Bloomberg Billionaires Index, and $254 billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX.[5][6]\n",
        "\n",
        "A member of the wealthy South African Musk family, Elon was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania, and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University. However, Musk dropped out after two days and, with his brother Kimbal, co-founded online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999, and, that same year Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal.\n",
        "\n",
        "In October 2002, eBay acquired PayPal for $1.5 billion, and that same year, with $100 million of the money he made, Musk founded SpaceX, a spaceflight services company. In 2004, he became an early investor in electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. In 2006, Musk helped create SolarCity, a solar-energy company that was acquired by Tesla in 2016 and became Tesla Energy. In 2013, he proposed a hyperloop high-speed vactrain transportation system. In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. The following year, Musk co-founded Neuralink—a neurotechnology company developing brain–computer interfaces—and the Boring Company, a tunnel construction company. In 2022, he acquired Twitter for $44 billion. He subsequently merged the company into newly created X Corp. and rebranded the service as X the following year. In March 2023, he founded xAI, an artificial intelligence company.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "summary_template = \"\"\"\n",
        "    given the information {information} about a person I want you to create:\n",
        "    1. A short summary\n",
        "    2. two interesting facts about them\n",
        "    \"\"\"\n",
        "\n",
        "summary_prompt_template = PromptTemplate(\n",
        "        input_variables=[\"information\"], template=summary_template\n",
        "    )\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "\n",
        "chain = summary_prompt_template | llm\n",
        "res = chain.invoke(input={\"information\": information})\n",
        "\n",
        "print(res)\n"
      ],
      "metadata": {
        "id": "nOg-YjG9vuO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}